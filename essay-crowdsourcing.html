<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Crowdsourcing, Classification, and Research Design</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="assets/css/style.css" />
</head>
<body>
<header class="site-header">
  <div class="container nav-bar">
    <div class="logo">Shiqi Ge</div>
    <nav class="nav-links">
      <a href="index.html">Home</a>
      <a href="projects.html">Portfolio</a>
      <a href="essays.html">Essays</a>
      <a href="about.html">About</a>
      <a href="mailto:your-email@example.com">Contact</a>
    </nav>
  </div>
</header>

<main class="section essay-main">
  <div class="container essay-container">
    <header class="essay-header">
      <p class="essay-label">Essay</p>
      <h1>Crowdsourcing, Classification, and Research Design</h1>
      <p class="essay-meta">
        Course: Digital Humanities · Word count: 2095<br />
        Author: Shiqi Ge
      </p>
    </header>

    <!-- 顶部插图占位符：整篇文章的主题配图 -->
    
    <figure class="essay-figure essay-figure-cover">
      <img src="assets/img/Essay3-pic1.png" alt="Profile page screenshot from platform Zooniverse" />
      <figcaption>My participation during the past month</figcaption>
    </figure>
    

    <!-- 可选：文章内部导航（如果你不想要可以整段删掉） -->
    <!--
    <nav class="essay-toc">
      <h2>Contents</h2>
      <ol>
        <li><a href="#intro">Confronting the materiality of AI</a></li>
        <li><a href="#invisible-labour">Invisible labour in AI systems</a></li>
        <li><a href="#bias-data-dignity">Bias, data dignity, and algorithmic harms</a></li>
        <li><a href="#swartz">Aaron Swartz and knowledge infrastructures</a></li>
        <li><a href="#parallels">Parallels between AI and academic publishing</a></li>
        <li><a href="#dh-practice">Digital humanities and responsible practice</a></li>
        <li><a href="#conclusion">Conclusion: towards a more equitable knowledge landscape</a></li>
      </ol>
    </nav>
    -->

    <article class="essay-body">

      <h2 id="process">The Process</h2>
      
      <p>
      When I first browsed Zooniverse, I noticed most of the featured projects were related to natural sciences, 
        such as wildlife images and astronomical data. I was initially hesitant to engage with these projects, 
        as I felt I lacked some necessary background knowledge. Thus I began with Transcribe 2026, which 
        consisted of two tasks: transcribing whole texts and identify names from scanned documents.
      </p>
      
      <p>
      It resembled what people typically associate with crowdsourcing projects, such as CAPTCHA. Working on it 
        was quite relaxing at first as typing letters demanded little cognitive effort. However, boredom soon set 
        in because I spent much time typing and double-checking to ensure accuracy, yet it didn’t reward me with 
        a sense of achievement proportionate to the effort invested. Consequently, I turned to another project, 
        <em>Sovraimpressioni</em>, which aimed to classify photographs by General Cesare Lomaglio.
      </p>

      <figure class="essay-figure">
        <img src="assets/img/Essay3-pic2.png" alt="Screenshot of transcribing a page of scanned document" />
        <figcaption>
          Some texts may not be very clear but can be guessed from its context
        </figcaption>
      </figure>
      
      <p>
      This more complex task required participants to choose from 4 catagory labels for each photo, including living beings, 
        environment, buildings, and vehicles, followed by 4 additional questions regarding identifiable faces, handwritten 
        notes, and the determination of location and time. Although each item took less time than the transcription task, 
        the cognitive demands were obviously higher. Many photographs contained limited contextual cues, such as portraits 
        against blank walls, partial rooftops, or small interior corners. I frequently faced uncertainty when distinguishing 
        between categories such as urban, rural, or other, as well as among different building types. Between guessing and 
        giving up, I decided to experiment with consulting ChatGPT and assess whether its suggestions could be reasonably applied.
      </p>

      <figure class="essay-figure">
        <img src="assets/img/Essay3-pic3.png" alt="An indoor photograph with minimal contextual cues" />
        <figcaption>
          An indoor environment almost impossible to classify the building type
        </figcaption>
      </figure>
      
      <p>
      Most photos were difficult to determine the location and period, as they lacked obvious landmark or time features. 
        I would also click “Done & Talk” to view comments. Some participants would still take a guess from their personal 
        experience, even if they were not sure. However, I’m not familiar with Italian history and landscape, so I simply 
        presented negative answer to most subjects. What’s more, when confronted with images that were almost impossible to 
        recognize the elements, such as wildlife photographs in another project Cameras for Conservation where only a limited 
        fragment of an animal was visible, I sometimes refreshed the page to be assigned with another photo, so that I could 
        avoid being forced to classify the photo.
      </p>
      
      <figure class="essay-figure">
        <img src="assets/img/Essay3-pic4.png" alt="Screenshots of comments page where participants told the time by costumes" />
        <figcaption>
          Participants usually infer the historical period based on the costumes.
        </figcaption>
      </figure>
      
      <p>
      Another point worth noting is that when confronted with images that were almost impossible to recognize the elements, such as 
        wildlife photographs in another project Cameras for Conservation where only a limited fragment of an animal was visible, I 
        sometimes refreshed the page to be assigned another photo, so that I could avoid being forced to classify the photo.
      </p>
      
      <figure class="essay-figure">
        <img src="assets/img/Essay3-pic5.png" alt="The animal’s fur partially obscured the lens, making species identification difficult" />
        <figcaption>
          Some photos only captured a limited, blurred fragment of an animal
        </figcaption>
      </figure>
      
   

      
      <h2 id="implications">Implications of My Contribution</h2>
      
      <p>
      At first glance, transcription and image classification appear to be minor and trivial work. However, their significance emerges 
        when situated within the broader transformation enacted by digital humanities projects.
      </p>
      
      <h3>Archive as database for searching and machine learning</h3>
      
      <p>
      Through transcription and annotation, archival artefacts are transformed into structured, machine-readable data. Handwritten fragments 
        become searchable text; photographs become categorised entries. Once rendered searchable and filterable, the archive no longer 
        functions solely as a static repository but as a dynamic database that enables large-scale retrieval, comparison, and pattern detection.
      </p>
      
      <p>
      This restructuring alters how knowledge circulates. Structured datasets move more easily across disciplinary boundaries and 
        become accessible not only to specialists trained in archival reading but also to researchers in adjacent fields and non-academic 
        audiences. In this sense, digitisation does not merely preserve historical materials; it reorganises the epistemic infrastructure 
        through which they can be queried and mobilised.
      </p>
      
      <p>
      At the same time, such structured annotations contribute to a broader technological ecology. Contemporary machine learning 
        systems depend on vast quantities of human-labelled data. The categorical distinctions applied in archival annotation—such 
        as identifying faces or distinguishing types of subjects—gradually accumulate into the semantic boundaries that algorithms 
        later learn to recognise. Automated perception is therefore grounded in distributed human judgement. My individual contribution 
        may be modest, yet it participates in this larger transformation from archival interpretation to computational capacity.
      </p>

      <figure class="essay-figure">
        <img src="assets/img/Essay3-pic6.png" alt="Screenshot when tagging people on the nose" />
        <figcaption>
          Tagging on identifiable faces
        </figcaption>
      </figure>
      
      <h3>Socio-Technical and Ethical Implications</h3>
      
      <p>
      However, reflecting on my contribution also raises ethical questions about labour, ownership, and recognition within crowdsourced 
        research. Crowdsourcing distributes intellectual work across many contributors, yet these contributors often occupy an ambiguous 
        position. They are neither formal authors nor traditional research participants; rather, they function as infrastructural labour 
        whose contributions enable knowledge production but remain largely invisible in the final output. As Marinova (2016) argues, by 
        relying on commercial crowdsourcing platforms, researchers risk reinforcing a legal and economic model grounded in precarious labor, 
        asymmetrical power relations, and the absence of worker protections.
      </p>
      
      <p>
      This raises questions of attribution and academic recognition. While the aggregated dataset may support scholarly publications or 
        institutional projects, individual contributors are rarely acknowledged beyond collective mention. The model depends on fragmenting 
        labour into micro-tasks, which increases efficiency but diffuses responsibility and obscures authorship.
      </p>
      
      <p>
      My decision to occasionally consult ChatGPT further complicated the system. Crowdsourcing is premised on distributed human judgement, 
        where independent participants contribute individual interpretations. By introducing an external AI system into the process, I 
        effectively hybridised that judgement. Although the platform does not explicitly prohibit such practices, the use of generative AI 
        raises questions about epistemic contamination: does the aggregation of responses still represent collective human assessment, or a 
        mixture of human and algorithmic inference?
      </p>
      
      <p>
      Additionally, crowdsourced work exists within broader political and economic structures. Online contributors frequently operate as 
        informal digital workers in precarious conditions, particularly across Global North–Global South inequalities. Even when compensation 
        is provided, payment structures may reflect global wage asymmetries (Du et al., 2024). Participating in such a system, even as a 
        volunteer or paid contributor, means becoming embedded within a wider economy of platform-mediated labour.
      </p>
      
      <p>
      Recognising these dimensions does not invalidate the value of crowdsourcing. Rather, it highlights that my contribution was not 
        neutral or purely technical. It was situated within a socio-technical system that shapes not only data, but also labour relations, 
        power structures, and the distribution of epistemic credit.
      </p>


      <h2 id="learning">What I Learned</h2>

      <h3>I. Skills and knowledge from crowdsourcing work</h3>
      
      <p>
      The most practical skill I developed emerged during the location identification task. According to the project guidelines, 
        I attempted reverse image searches for selected photographs, trying to locate identical or similar images that might have appeared 
        in published articles or websites. In one rare case, I successfully traced the exact same image to an online article that explicitly 
        mentioned the shooting location. Following the instructions, I then used Google Maps to identify the precise geographical coordinates 
        of that site and recorded the metadata accordingly.
      </p>
      
      <p>
      Yet this successful identification occurred only once. For the majority of images that were randomly assigned to me, no online traces 
        could be found. Many photographs depicted rural landscapes, undeveloped terrain, or visually indistinct environments that offered no 
        identifiable landmarks. In these cases, reverse searching yielded no results, and there were insufficient contextual cues to determine 
        a precise location.
      </p>
      
      <p>
      Honestly from my experience, participating in the crowdsourcing project offered limited opportunities for the development of critical 
        skills. The nature of crowdsourcing limits participants to repetitive and low-level tasks, requiring minimal intellectual engagement.
      </p>
      
      <h3>II. Surprising Finding: Fragmentation and the Loss of Context</h3>
      
      <p>
      Before participating in this project, I regarded crowdsourcing as an ingenious model. It appeared to mobilize collective intelligence 
        in a remarkably efficient way, distributing repetitive and labor-intensive tasks across a large number of participants. In doing so, 
        it not only accelerates data collection but also allows ambiguous samples to be evaluated through multiple perspectives. By aggregating 
        majority opinions, the system increases the statistical likelihood of arriving at a reliable outcome.
      </p>
      
      <p>
      However, during the process, I became aware of a structural limitation. The fragmentation of tasks detaches each sample from its 
        historical and material context. Items are presented as isolated units rather than as parts of a continuous archive. For example, 
        while annotating photographs, I encountered two images at different moments in the workflow that were clearly taken at the same 
        location, from nearly the same angle, and within a very short time interval.
        In both images, a particular individual appeared: in the first photograph, she stood closer to the camera; in the second, she had 
        moved farther down the road. Although her figure became smaller and less distinct, her posture and the object she was carrying allowed 
        me to infer continuity between the two shots. Taken together, these contextual cues strongly suggested that the images were captured 
        within minutes of one another.
      </p>

      <figure class="essay-figure">
        <img src="assets/img/Essay3-pic7.png" alt="" />

      </figure>
      <figure class="essay-figure">
        <img src="assets/img/Essay3-pic8.png" alt="" />
        <figcaption>
          These two photos were highly related
        </figcaption>
      </figure>
      
      <p>
      Yet within the platform, the two photographs were separated and assigned as independent tasks. If a researcher were physically examining 
        a stack of prints, these images would most likely appear consecutively, making their relationship immediately visible. In such a case, 
        consistent labeling would almost be self-evident. On the platform, however, participants may encounter only one of the two images. Their 
        judgments are therefore formed in isolation. Only by coincidence did I annotate both photographs and retain the memory of the first when 
        encountering the second.
      </p>
      
      <p>
      This fragmentation introduces a loss of contextual continuity. When items are detached from adjacent materials, participants cannot rely 
        on relational inference. As a result, two nearly identical images may receive different labels, simply because they are evaluated 
        independently. Slight compositional shifts—such as the relative distance of a person from the camera or the presence of an additional 
        figure—may alter interpretation. What would likely produce consistent annotations in a physically contiguous archive may instead 
        generate divergent outcomes in a fragmented digital workflow.
      </p>
      
      <p>
      In this sense, crowdsourcing does not merely distribute labor; it restructures knowledge production rules. It replaces contextual 
        interpretation with isolated micro-judgment. What is lost is the internal coherence between related samples; what is gained is the 
        aggregation of independent, decontextualized evaluations. The system values statistical independence over relational understanding.
      </p>
      
      <p>
      This experience led me to reconsider my earlier enthusiasm. Crowdsourcing remains powerful and efficient, yet its design may sacrifice 
        archival context for scalability. The very mechanism that enables large-scale participation simultaneously transforms how knowledge 
        is constructed.
      </p>

      <h2 id="application">Application to My Own Work</h2>
      
      <h3>Research Design: a participant-friendly working environment</h3>
      
      <p>
      The classification work reinforced how strongly structured data depends on predefined categories. When identifying whether images 
        contained “dogs,” “military personnel,” or “other living beings,” I relied on visual conventions shaped by the project’s thematic 
        focus. The prominence of horses and dogs reflected their historical relevance within wartime military contexts. Other animals, 
        though present, were analytically secondary.
      </p>
      
      <p>
      This experience clarified that classification is inseparable from research design. Categories do not aim to exhaust reality; they 
        foreground what is meaningful within a particular analytical framework. Effective annotation therefore requires alignment between 
        scholarly intention and system structure.
      </p>
      
      <p>
      The key elements for designing a project are more than pre-catagorization. As Deng and Joshi (2016) revealed, integrating task 
        characteristics, digital governance, and individual work values into an extended Job Characteristics Theory framework helps 
        explain both initial and continued participation in micro-task crowdsourcing. They also conceptualize participation in micro-task 
        crowdsourcing as shaped by the interplay of work context, task characteristics, worker needs, and digital control mechanisms, 
        which collectively influence hedonic fulfillment, work value realization, and satisfaction. In addition, performance monitoring, 
        incentive structures, and ethical regulation contribute significantly to the reliability of crowdsourcing systems (Gadiraju et al., 
        2015); however, these mechanisms are less explicitly foregrounded in the Zooniverse environment.
      </p>
      
      <h3>Extending crowdsourcing to my audiovisual research</h3>
      
      <p>
      Crowdsourcing is likely to become a significant methodological resource in my future research on audiovisual media. Video-based 
        studies often involve extensive material: thousands of frames, multimodal elements, and complex symbolic structures. Distributed 
        annotation could assist in identifying visual motifs, coding narrative segments, or tagging cultural symbols.
      </p>
      
      <p>
      More importantly, participant responses themselves can function as research data. Short audiovisual excerpts could be presented 
        to contributors who select from predefined emotional or interpretive categories. Aggregated responses would then form a large-scale 
        dataset of audience reception, enabling quantitative analysis of emotional attribution, interpretive convergence, and cross-cultural 
        divergence.
      </p>
      
      <p>
      In transnational film research, such an approach would be particularly productive. Identical sequences may generate distinct 
        emotional or cultural associations across linguistic and cultural contexts. Systematically comparing these patterns could offer 
        empirical grounding for discussions of global reception and meaning production. In this sense, crowdsourcing extends beyond 
        distributed labour; it becomes a framework for modeling collective interpretation.
      </p>
      
      <p>
      However, the validity of such applications would depend on careful category construction and robust quality control. Emotional 
        responses are complex and context-sensitive. Overly rigid predefined options risk flattening affective nuance. Therefore, 
        research design must balance structure with interpretive openness.
      </p>
      
      <p>
      As both a researcher in audiovisual media and a potential practitioner in mass communication, I see strong potential in developing 
        crowdsourcing models that combine structured annotation, participant-centered design, and methodological reflexivity. Rather 
        than treating data production as neutral infrastructure, future projects must recognize it as an epistemic and organizational 
        practice that shapes knowledge itself.
      </p>

      
      <!-- 插图占位符：数字人文实践 / 课堂或作品截图 -->
      <!--
      <figure class="essay-figure">
        <img src="assets/img/essays/dh-practice.jpg" alt="Screenshot of a digital humanities project interface" />
        <figcaption>Figure 4. Situating digital humanities practice within broader infrastructures and ethics.</figcaption>
      </figure>
      -->

      



      
      <h2 id="references">References</h2>

      <ul class="references-list">
        <li>
            Deng, Xuefei (Nancy) and Joshi, K.D. (2016) “Why Individuals Participate in Micro-task Crowdsourcing Work Environment: Revealing Crowdworkers’ Perceptions,” 
            <em>Journal of the Association for Information Systems</em>, 17(10). 
            DOI: 10.17705/1jais.00441
        </li>
    
        <li>
            Du, S., Babalola, M.T., D’Cruz, P. et al. (2024) “The Ethical, Societal, and Global Implications of Crowdsourcing Research”. 
            <em>Journal of Business Ethics</em>, 193, pp. 1–16. 
            https://doi.org/10.1007/s10551-023-05604-9
        </li>
    
        <li>
            Gadiraju, U., Demartini, G., Kawase, R. and Dietze, S. (2015) “Human Beyond the Machine: Challenges and Opportunities of Microtask Crowdsourcing,” 
            <em>IEEE Intelligent Systems</em>, 30(4), pp. 81–85. 
            doi:10.1109/MIS.2015.66
        </li>
    
        <li>
            Marinova, D.M. (2016) ‘On the Use of Crowdsourcing Labor Markets in Research’, 
            <em>Perspectives on Politics</em>, 14(2), pp. 422–431. 
            doi:10.1017/S1537592716000104
        </li>
      </ul>
    </article>
  </div>
</main>


<footer class="site-footer">
  <div class="container">
    <p>© <span id="year"></span> Shiqi Ge · Digital Arts &amp; Humanities</p>
  </div>
  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>
</footer>
</body>
</html>
