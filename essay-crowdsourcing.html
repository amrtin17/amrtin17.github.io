<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Crowdsourcing, Classification, and Research Design</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="assets/css/style.css" />
</head>
<body>
<header class="site-header">
  <div class="container nav-bar">
    <div class="logo">Shiqi Ge</div>
    <nav class="nav-links">
      <a href="index.html">Home</a>
      <a href="projects.html">Portfolio</a>
      <a href="essays.html">Essays</a>
      <a href="about.html">About</a>
      <a href="mailto:your-email@example.com">Contact</a>
    </nav>
  </div>
</header>

<main class="section essay-main">
  <div class="container essay-container">
    <header class="essay-header">
      <p class="essay-label">Essay</p>
      <h1>Crowdsourcing, Classification, and Research Design</h1>
      <p class="essay-meta">
        Course: Digital Humanities · Word count: 2041<br />
        Author: Shiqi Ge
      </p>
    </header>

    <!-- 顶部插图占位符：整篇文章的主题配图 -->
    
    <figure class="essay-figure essay-figure-cover">
      <img src="assets/img/Essay3-pic1.png" alt="Profile page screenshot from platform Zooniverse" />
      <figcaption>My participation during the past month</figcaption>
    </figure>
    

    <!-- 可选：文章内部导航（如果你不想要可以整段删掉） -->
    <!--
    <nav class="essay-toc">
      <h2>Contents</h2>
      <ol>
        <li><a href="#intro">Confronting the materiality of AI</a></li>
        <li><a href="#invisible-labour">Invisible labour in AI systems</a></li>
        <li><a href="#bias-data-dignity">Bias, data dignity, and algorithmic harms</a></li>
        <li><a href="#swartz">Aaron Swartz and knowledge infrastructures</a></li>
        <li><a href="#parallels">Parallels between AI and academic publishing</a></li>
        <li><a href="#dh-practice">Digital humanities and responsible practice</a></li>
        <li><a href="#conclusion">Conclusion: towards a more equitable knowledge landscape</a></li>
      </ol>
    </nav>
    -->

    <article class="essay-body">

      <h2 id="process">The Process</h2>
      
      <p>
      When I first browsed Zooniverse, I noticed most of the featured projects were related to natural sciences, 
        such as wildlife images and astronomical data. I was initially hesitant to engage with these projects, 
        as I felt I lacked some necessary background knowledge. Thus I began with Transcribe 2026, which 
        consisted of two tasks: transcribing whole texts and identify names from scanned documents.
      </p>
      
      <p>
      It resembled what people typically associate with crowdsourcing projects, such as CAPTCHA. Working on it 
        was quite relaxing at first as typing letters demanded little cognitive effort. However, boredom soon set 
        in because I spent much time typing and double-checking to ensure accuracy, yet it didn’t reward me with 
        a sense of achievement proportionate to the effort invested. Consequently, I turned to another project, 
        <em>Sovraimpressioni</em>, which aimed to classify photographs by General Cesare Lomaglio.
      </p>

      <figure class="essay-figure">
        <img src="assets/img/Essay3-pic2.png" alt="Screenshot of transcribing a page of scanned document" />
        <figcaption>
          Some texts may not be very clear but can be guessed from its context
        </figcaption>
      </figure>
      
      <p>
      This more complex task required participants to choose from 4 catagory labels for each photo, including living beings, 
        environment, buildings, and vehicles, followed by 4 additional questions regarding identifiable faces, handwritten 
        notes, and the determination of location and time. Although each item took less time than the transcription task, 
        the cognitive demands were obviously higher. Many photographs contained limited contextual cues, such as portraits 
        against blank walls, partial rooftops, or small interior corners. I frequently faced uncertainty when distinguishing 
        between categories such as urban, rural, or other, as well as among different building types. Between guessing and 
        giving up, I decided to experiment with consulting ChatGPT and assess whether its suggestions could be reasonably applied.
      </p>

      <figure class="essay-figure">
        <img src="assets/img/Essay3-pic3.png" alt="An indoor photograph with minimal contextual cues" />
        <figcaption>
          An indoor environment almost impossible to classify the building type
        </figcaption>
      </figure>
      
      <p>
      Most photos were difficult to determine the location and period, as they lacked obvious landmark or time features. 
        After classifying one photo, I would also view comments. Some participants would still take a guess from their personal 
        experience, even if they were not sure. However, I’m not familiar with Italian history and landscape, so I simply 
        presented negative answer to most subjects.
      </p>
      
      <figure class="essay-figure">
        <img src="assets/img/Essay3-pic4.png" alt="Screenshots of comments page where participants told the time by costumes" />
        <figcaption>
          Participants usually infer the historical period based on the costumes.
        </figcaption>
      </figure>
      
      <p>
      Another point worth noting is that when confronted with images that were almost impossible to recognize the elements, such as 
        wildlife photographs in another project Cameras for Conservation where only a limited fragment of an animal was visible, I 
        sometimes refreshed the page to be assigned another photo, so that I could avoid being forced to classify the photo.
      </p>
      
      <figure class="essay-figure">
        <img src="assets/img/Essay3-pic5.png" alt="The animal’s fur partially obscured the lens, making species identification difficult" />
        <figcaption>
          Some photos only captured a limited, blurred fragment of an animal
        </figcaption>
      </figure>
      
   

      
      <h2 id="implications">Implications of My Contribution</h2>
      
      <p>
      At first glance, transcription and image classification appear to be minor and trivial work. However, their significance emerges 
        when situated within the broader transformation enacted by digital humanities projects.
      </p>
      
      <h3>Archive as database for searching and machine learning</h3>
      
      <p>
      Through transcription and annotation, archival artefacts are transformed into structured, machine-readable data. Handwritten fragments 
        become searchable text; photographs become categorised entries. Once rendered searchable and filterable, the archive no longer 
        functions solely as a static repository but as a dynamic database that enables large-scale retrieval, comparison, and pattern detection.
      </p>
      
      <p>
      This restructuring alters how knowledge circulates. Structured datasets move more easily across disciplinary boundaries and 
        become accessible not only to specialists trained in archival reading but also to researchers in adjacent fields and non-academic 
        audiences. In this sense, digitisation does not merely preserve historical materials; it reorganises the epistemic infrastructure 
        through which they can be queried and mobilised.
      </p>
      
      <p>
      At the same time, such structured annotations contribute to a broader technological ecology. Contemporary machine learning 
        systems depend on vast quantities of human-labelled data. The categorical distinctions applied in archival annotation—such 
        as identifying faces or distinguishing types of subjects—gradually accumulate into the semantic boundaries that algorithms 
        later learn to recognise. Automated perception is therefore grounded in distributed human judgement. My individual contribution 
        may be modest, yet it participates in this larger transformation from archival interpretation to computational capacity.
      </p>

      <figure class="essay-figure">
        <img src="assets/img/Essay3-pic6.png" alt="Screenshot when tagging people on the nose" />
        <figcaption>
          Tagging on identifiable faces
        </figcaption>
      </figure>
      
      <h3>Socio-Technical and Ethical Implications</h3>
      
      <p>
      However, reflecting on my contribution also raises ethical questions about labour, ownership, and recognition within crowdsourced 
        research. Crowdsourcing distributes intellectual work across many contributors, yet these contributors often occupy an ambiguous 
        position. They are neither formal authors nor traditional research participants; rather, they function as infrastructural labour 
        whose contributions enable knowledge production but remain largely invisible in the final output. As Marinova (2016) argues, by 
        relying on commercial crowdsourcing platforms, researchers risk reinforcing a legal and economic model grounded in precarious labor, 
        asymmetrical power relations, and the absence of worker protections.
      </p>
      
      <p>
      This raises questions of attribution and academic recognition. While the aggregated dataset may support scholarly publications or 
        institutional projects, individual contributors are rarely acknowledged beyond collective mention. The model depends on fragmenting 
        labour into micro-tasks, which increases efficiency but diffuses responsibility and obscures authorship.
      </p>
      
      <p>
      My decision to occasionally consult ChatGPT further complicated the system. Crowdsourcing is premised on distributed human judgement, 
        where independent participants contribute individual interpretations. By introducing an external AI system into the process, I 
        effectively hybridised that judgement. Although the platform does not explicitly prohibit such practices, the use of generative AI 
        raises questions about epistemic contamination: does the aggregation of responses still represent collective human assessment, or a 
        mixture of human and algorithmic inference?
      </p>
      
      <p>
      Additionally, crowdsourced work exists within broader political and economic structures. Online contributors frequently operate as 
        informal digital workers in precarious conditions, particularly across Global North–Global South inequalities. Even when compensation 
        is provided, payment structures may reflect global wage asymmetries (Du et al., 2024). Participating in such a system, even as a 
        volunteer or paid contributor, means becoming embedded within a wider economy of platform-mediated labour.
      </p>
      
      <p>
      Recognising these dimensions does not invalidate the value of crowdsourcing. Rather, it highlights that my contribution was not 
        neutral or purely technical. It was situated within a socio-technical system that shapes not only data, but also labour relations, 
        power structures, and the distribution of epistemic credit.
      </p>


      <h2 id="learning">What I Learned</h2>

      <h3>I. Skills and knowledge from crowdsourcing work</h3>
      
      <p>
      The most practical skill I developed emerged during the location identification task. According to the project guidelines, 
        I attempted reverse image searches for selected photographs, trying to locate identical or similar images that might have appeared 
        in published articles or websites. In one rare case, I successfully traced the exact same image to an online article that explicitly 
        mentioned the shooting location. Following the instructions, I then used Google Maps to identify the precise geographical coordinates 
        of that site and recorded the metadata accordingly.
      </p>
      
      <p>
      Yet this successful identification occurred only once. For the majority of images that were randomly assigned to me, no online traces 
        could be found. Many photographs depicted rural landscapes, undeveloped terrain, or visually indistinct environments that offered no 
        identifiable landmarks. In these cases, reverse searching yielded no results, and there were insufficient contextual cues to determine 
        a precise location.
      </p>
      
      <p>
      Honestly from my experience, participating in the crowdsourcing project offered limited opportunities for the development of critical 
        skills. The nature of crowdsourcing limits participants to repetitive and low-level tasks, requiring minimal intellectual engagement.
      </p>
      
      <h3>II. Surprising Finding: Fragmentation and the Loss of Context</h3>
      
      <p>
      Before participating in this project, I regarded crowdsourcing as an ingenious model. It appeared to mobilize collective intelligence 
        efficiently, distributing repetitive tasks across large numbers of contributors. By aggregating multiple perspectives, especially 
        on ambiguous samples, the system increases the statistical likelihood of reliable outcomes.
      </p>
      
      <p>
      However, during the process, I became aware of a structural limitation: task fragmentation detaches each item from its archival context. 
        Materials are presented as isolated units rather than as parts of a continuous sequence. While annotating photographs, I encountered 
        two images at different moments in the workflow that were clearly taken at the same location, from nearly the same angle, and within 
        a short time interval. In both images, the same woman appeared—closer to the camera in the first, farther down the road in the second. 
        Her posture and the object she carried suggested continuity, indicating that the photographs were captured only minutes apart.
      </p>
      
      <p>
      Yet on the platform, the two photographs were separated and assigned as independent tasks. If a researcher were examining a stack of 
        physical prints, the images would likely appear side by side, and their connection would be immediately apparent. In that situation, 
        consistent labeling would feel almost obvious. Online, however, participants may encounter only one of the images, with no access 
        to adjacent materials. Their judgments are formed in isolation. It was only by chance that I annotated both photographs and remembered 
        the first when I came across the second.
      </p>

      <figure class="essay-figure">
        <img src="assets/img/Essay3-pic7.png" alt="" />

        <img src="assets/img/Essay3-pic8.png" alt="" />
        <figcaption>
          These two photos were highly related
        </figcaption>
      </figure>
      
      <p>
      This separation disrupts contextual continuity. When materials are detached from their surrounding items, participants lose the ability 
        to draw on relational cues. As a result, two closely related images may receive different labels simply because they are evaluated 
        independently. Even small compositional differences, such as the changing distance of a figure from the camera or the presence of 
        an additional person, can shift interpretation. In a physically contiguous archive, these images would likely produce consistent 
        annotations. Within a fragmented digital workflow, divergence becomes more probable.
      </p>
      
      <p>
      Crowdsourcing therefore does more than distribute labor. It reshapes how interpretation takes place. Contextual reading gives way to 
        isolated micro-level judgments, and coherence between related materials becomes secondary to the aggregation of independent responses. 
        The system prioritizes statistical independence rather than relational understanding.
      </p>
      
      <p>
      This experience led me to rethink my earlier enthusiasm. Crowdsourcing remains efficient and powerful, but its design can privilege 
        scalability over archival continuity. The very structure that enables large-scale participation also alters the conditions under 
        which knowledge is produced.
      </p>

      <h2 id="application">Application to My Own Work</h2>
      
      <h3>Research Design: a participant-friendly working environment</h3>
      
      <p>
      The classification work reinforced how strongly structured data depends on predefined categories. When identifying whether images 
        contained “dogs,” “military personnel,” or “other living beings,” I relied on visual conventions shaped by the project’s thematic 
        focus. The prominence of horses and dogs reflected their historical relevance within wartime military contexts. Other animals, 
        though present, were analytically secondary.
      </p>
      
      <p>
      This experience clarified that classification is inseparable from research design. Categories do not aim to exhaust reality; they 
        foreground what is meaningful within a particular analytical framework. Effective annotation therefore requires alignment between 
        scholarly intention and system structure.
      </p>
      
      <p>
      The key elements for designing a project are more than pre-catagorization. As Deng and Joshi (2016) revealed, integrating task 
        characteristics, digital governance, and individual work values into an extended Job Characteristics Theory framework helps 
        explain both initial and continued participation in micro-task crowdsourcing. They also conceptualize participation in micro-task 
        crowdsourcing as shaped by the interplay of work context, task characteristics, worker needs, and digital control mechanisms, 
        which collectively influence hedonic fulfillment, work value realization, and satisfaction. In addition, performance monitoring, 
        incentive structures, and ethical regulation contribute significantly to the reliability of crowdsourcing systems (Gadiraju et al., 
        2015); however, these mechanisms are less explicitly foregrounded in the Zooniverse environment.
      </p>
      
      <h3>Extending crowdsourcing to my audiovisual research</h3>
      
      <p>
      Crowdsourcing is likely to become a significant methodological resource in my future research on audiovisual media. Video-based 
        studies often involve extensive material: thousands of frames, multimodal elements, and complex symbolic structures. Distributed 
        annotation could assist in identifying visual motifs, coding narrative segments, or tagging cultural symbols.
      </p>
      
      <p>
      More importantly, participant responses themselves can function as research data. Short audiovisual excerpts could be presented 
        to contributors who select from predefined emotional or interpretive categories. Aggregated responses would then form a large-scale 
        dataset of audience reception, enabling quantitative analysis of emotional attribution, interpretive convergence, and cross-cultural 
        divergence.
      </p>
      
      <p>
      In transnational film research, such an approach would be particularly productive. Identical sequences may generate distinct 
        emotional or cultural associations across linguistic and cultural contexts. Systematically comparing these patterns could offer 
        empirical grounding for discussions of global reception and meaning production. In this sense, crowdsourcing extends beyond 
        distributed labour; it becomes a framework for modeling collective interpretation.
      </p>
      
      <p>
      However, the validity of such applications would depend on careful category construction and robust quality control. Emotional 
        responses are complex and context-sensitive. Overly rigid predefined options risk flattening affective nuance. Therefore, 
        research design must balance structure with interpretive openness.
      </p>
      
      <p>
      As both a researcher in audiovisual media and a potential practitioner in mass communication, I see strong potential in developing 
        crowdsourcing models that combine structured annotation, participant-centered design, and methodological reflexivity. Rather 
        than treating data production as neutral infrastructure, future projects must recognize it as an epistemic and organizational 
        practice that shapes knowledge itself.
      </p>

      
      <!-- 插图占位符：数字人文实践 / 课堂或作品截图 -->
      <!--
      <figure class="essay-figure">
        <img src="assets/img/essays/dh-practice.jpg" alt="Screenshot of a digital humanities project interface" />
        <figcaption>Figure 4. Situating digital humanities practice within broader infrastructures and ethics.</figcaption>
      </figure>
      -->

      



      
      <h2 id="references">References</h2>
      <ul class="reference-list">
        <li>
          Deng, Xuefei (Nancy) and Joshi, K.D. (2016) ‘Why Individuals Participate in Micro-task 
          Crowdsourcing Work Environment: Revealing Crowdworkers’ Perceptions,’ <em>Journal of the Association 
          for Information Systems</em>, 17(10). DOI: 10.17705/1jais.00441.(Accessed: 25 February 2026)
        </li>
        <li>
          Du, S., Babalola, M.T., D’Cruz, P. et al. (2024) ‘The Ethical, Societal, and Global Implications of Crowdsourcing Research’. 
          <em>Journal of Business Ethics</em>, 193, pp. 1–16. https://doi.org/10.1007/s10551-023-05604-9.(Accessed: 22 February 2026)
        </li>
        <li>
          Gadiraju, U., Demartini, G., Kawase, R. and Dietze, S. (2015) ‘Human Beyond the Machine: Challenges and Opportunities of Microtask 
          Crowdsourcing,’ <em>IEEE Intelligent Systems</em>, 30(4), pp. 81–85. doi:10.1109/MIS.2015.66.(Accessed: 22 February 2026)
        </li>
        <li>
          Marinova, D.M. (2016) ‘On the Use of Crowdsourcing Labor Markets in Research’, <em>Perspectives on Politics</em>, 14(2), 
          pp. 422–431. doi:10.1017/S1537592716000104.(Accessed: 20 February 2026)
        </li>
       
      </ul>
    </article>
  </div>
</main>


<footer class="site-footer">
  <div class="container">
    <p>© <span id="year"></span> Shiqi Ge · Digital Arts &amp; Humanities</p>
  </div>
  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>
</footer>
</body>
</html>
